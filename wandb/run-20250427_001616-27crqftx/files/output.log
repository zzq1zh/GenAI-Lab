Traceback (most recent call last):
  File "/oscar/home/zliu328/GenAI-Lab/train_bi.py", line 163, in <module>
    trainer.train()
  File "/users/zliu328/miniconda3/envs/genai/lib/python3.9/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/users/zliu328/miniconda3/envs/genai/lib/python3.9/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/users/zliu328/miniconda3/envs/genai/lib/python3.9/site-packages/transformers/trainer.py", line 3782, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/users/zliu328/miniconda3/envs/genai/lib/python3.9/site-packages/accelerate/accelerator.py", line 2454, in backward
    loss.backward(**kwargs)
  File "/users/zliu328/miniconda3/envs/genai/lib/python3.9/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/users/zliu328/miniconda3/envs/genai/lib/python3.9/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Rank 1 successfully reached monitoredBarrier, but received errors while waiting for send/recv from rank 0. Please check rank 0 logs for faulty rank.
 Original exception:
[../third_party/gloo/gloo/transport/tcp/pair.cc:534] Connection closed by peer [172.20.226.2]:1461